---
title: "Che on OpenShift: Admin Guide"
keywords: openshift, configuration, admin guide
tags: [installation, kubernetes]
sidebar: user_sidebar
permalink: openshift-admin-guide.html
folder: setup-openshift
---


[id="examples"]
== Examples

All examples use `the kubectl command'. OpenShift administrators should use `the oc command`.

[id="ram"]
== RAM prerequisites

*Single User*

3 GB of RAM is required for single-user Che on OpenShift.

Single-user Che uses RAM in this distribution:

* Che server pod uses up to 1 GB of RAM. The initial request for RAM is 256 MB. The Che server pod rarely uses more than 800 MB RAM.
* Workspaces: 2 GB of RAM per workspace. 

*Multi-User*

You must have at least 5 GB of RAM to run multi-user Che. The Keycloak authorization server and PostgreSQL database require the extra RAM. Multi-user Che uses RAM in this distribution:

* Che server: approximately 750 MB 
* Keycloak: approximately 1 GB 
* PostgreSQL: approximately 515 MB 
* Workspaces: 2 GB of RAM per workspace. The total workspace RAM depends on the size of the workspace runtime(s) and the number of concurrent workspace pods. 

[id="resource-allocation-and-quotas"]
== Requirements for resource allocation and quotas

Workspace pods are created in the account of the user who deploys Che.  The user needs enough quota for RAM, CPU, and storage to create that pod. 

[id="who-creates-workspace-objects"]
== Setting up the project workspace

Workspace objects are created differently depending on the configuration. Eclipse Che currently supports two different configurations: 

* Single OpenShift project   

* Multi OpenShift project   

*Setting Up A Single OpenShift Project*

Procedure

. Define the service account used to create workspace objects with the environment variable
`CHE_OPENSHIFT_SERVICEACCOUNTNAME`.
. To ensure this service account is visible to the Che server, put the service and the Che server in the same namespace.
. Give the service account permissions to create and edit OpenShift resources.
. If the developer needs to create an object outside of the service accounts bound namespace, give the service account cluster-admin rights by running this command:
+
----
oc adm policy add-cluster-role-to-user self-provisioner system:serviceaccount:eclipse-che:che
----
[NOTE]
====
Eclipse-Che is Che namespace.
====

*Setting up a multi OpenShift project*

Procedure

. To create workspace objects in different namespaces for each user, set `NULL_CHE_INFRA_OPENSHIFT_PROJECT to NULL`

. To create resources on behalf of the currently logged-in user, use user’s OpenShift tokens.

[id="storage-overview"]
== How the Che server uses PVCs and PVs for storage 

Che server, Keycloak, PostgreSQL, and workspace pods use Persistent Volume Claims (PVCs), which are bound to the physical Persistent Volumes (PVs) with https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes[ReadWriteOnce access mode]. When the deployment YAML files run, they define the Che PVCs. You can configure link:#che-workspaces-storage[workspace PVC] access mode and claim size with Che deployment environment variables.

[id="che-infrastructure-storage"]
== Storage requirements for Che infrastructure

The storage requirements for Che infrastructure:

* Che server: 1 GB to store logs and initial workspace stacks
* Keycloak: 2 PVCs, 1 GB each to store logs and Keycloak data
* PostgreSQL: 1 GB PVC to store database

[id="che-workspaces-storage"]
== Storage strategies for Che workspaces

The workspace PVC strategy is configurable:

[width="100%",cols="25%,25%,25%,25%",options="header",]
|===
|strategy |details |pros |cons
|*unique (default)* |PVC per workspace |Storage isolation |An undefined number of PVs is required
|*common* |

One PVC for all workspaces

Sub-paths pre-created
 
|easy to manage and control storage

no need to recycle PVs when pod with pvc is deleted |ws pods should all be in one namespace
|===

== Unique PVC strategy

To define the unique strategy:

* `CHE_INFRA_KUBERNETES_PVC_STRATEGY` is set to `unique`.

Every workspace gets its own PVC, which means a workspace PVC is created when a workspace starts for the first time. Workspace PVC is deleted when a corresponding workspace is deleted.

[id="common-pvc-strategy"]
== Common PVC Strategy

*How the common PVC strategy works*

All workspaces use the same PVC to store data declared in their volumes (projects and workspace logs by default and whatever additional link:volumes[volumes] that a user can define.)

A PV that is bound to PVC `che-claim-workspace` will have the following structure:

----
pv0001
  workspaceid1
  workspaceid2
  workspaceidn
    che-logs projects <volume1> <volume2>
----

Volumes can be anything that a user defines as volumes for workspace machines (volume name is equal to directory name in `${PV}/${ws-id}`).

When a workspace is deleted, a corresponding subdirectory (`${ws-id}`) is deleted in the PV directory.

== Enabling a common strategy

If you have already deployed Che with unique strategy:

* Set `CHE_INFRA_KUBERNETES_PVC_STRATEGY` to `common` in dc/che.

If applying che-server-template.yaml:

* Pass `-p CHE_INFRA_KUBERNETES_PVC_STRATEGY=common` to `oc new-app` command. 

[NOTE]
====
. Pre 1.6 Kubernetes, you need to set `CHE_INFRA_KUBERNETES_PVC_PRECREATE__SUBPATHS` to `true`.

. In Kubernetes 1.6 and higher, setting this variable to true is not a requirement.  
====

*Restrictions on using Common PVC Strategy*

When a common strategy is used, and a workspace PVC access mode is RWO, only one Kubernetes node can simultaneously use PVC.  If there are several nodes, a common strategy can still be used, but in this case, workspace PVC access mode should be RWM, ie multiple nodes should be able to use this PVC simultaneously (in fact, you may sometimes have some luck and all workspaces will be scheduled on the same node). You can change access mode for workspace PVCs by passing environment variable `CHE_INFRA_KUBERNETES_PVC_ACCESS_MODE=ReadWriteMany` to che deployment either when initially deploying Che or through che deployment update.

Another restriction is that only pods in the same namespace can use the same PVC, thus, `CHE_INFRA_KUBERNETES_PROJECT` environment variable should not be empty - it should be either Che server namespace (in this case objects can be created with che SA) or a dedicated namespace (token or username/password need to be used).

[id="update"]
== Updating your Che deployment

To update  Che deployment:

. Change the image tag: 
+
You can change the image tag in one of the following ways: 

* Execute `kubeclt edit dc/che`.
* In the OpenShift web console: *deployments > edit yaml > image:tag*.
* Using the Docker service: `kubectl set image dc/che che=eclipse/che-server:${VERSION} --source=docker`.

. Update Keycloak and PostgreSQL deployments (optional):

* eclipse/che-keycloak
* eclipse/che-postgres
+
You can get the list of available versions at https://github.com/eclipse/che/tags[Che GitHub page].

. Change the pull policy (optional):
+
To change the pull policy, do one of the following:

* add  `--set cheImagePullPolicy=IfNotPresent` to link:openshift-multi-user[Che deployment].
* manually edit `dc/che` after deployment. 

The default pull policy is Always. The default tag is `nightly`. This tag sets the image pull policy to Always and triggers a new deployment with a newer image, if available.


[id="scalability"]
== Scalability

To run more workspaces, https://kubernetes.io/docs/concepts/architecture/nodes/#management[add more nodes to your Kubernetes cluster].  An error message is returned when the system is out of resources.

[id="gdpr"]
== GDPR

To delete data or request the admininistrator to delete data, run this command with the user or adminstrator token:

----
$ curl -X DELETE http://che-server/api/user/{id}
----


[id="debug-mode"]
== Debug mode

To run Che Server in debug mode, set the following environment variable in the Che deployment to `true` (default is `false`):

`CHE_DEBUG_SERVER=true`

[id="private-docker-registries"]
== Private Docker Registries

Refer to https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/[Kubernetes documentation]

[id="che-server-logs"]
== Che Server Logs

The PVC `che-data-volume` is https://github.com/eclipse/che/blob/master/deploy/kubernetes/kubectl/che-kubernetes.yaml#L26[created] and bound to a PV after Eclipse Che deploys to Kubernetes. Logs are persisted in a PV.

To retrieve logs, do one of the following:

* `kubectl get log dc/che`
* `kubectl describe pvc che-data-claim`, find PV it is bound to, then `oc describe pv $pvName`, you will get a local path with logs directory. Be careful with permissions for that directory, since once changed, Che server wont be able to write to a file
* in Kubernetes web console, eclipse-che namespace, *pods > che-pod > logs*.

It is also possible to configure Che master not to store logs, but produce JSON encoded logs to output instead. It may be used to collect logs by systems such as Logstash. To configure JSON logging instead of plain text environment variable `CHE_LOGS_APPENDERS_IMPL` should have value `json`. See more at link:logging[logging docs].

[id="workspace-logs"]
== Workspace Logs

Workspace logs are stored in an PV bound to `che-claim-workspace` PVC. Workspace logs include logs from workspace agent, link:what-are-workspaces.html#bootstrapper[bootstrapper] and other agents if applicable.

[id="che-master-states"]
== Che Master States

The Che Master has three possible states:

* `RUNNING`
* `PREPARING_TO_SHUTDOWN`
* `READY_TO_SHUTDOWN`

The `PREPARING_TO_SHUTDOWN` state means that no new workspace startups are allowed. This situation can cause two different results: 

* If your infrastructure does not support workspace recovery, all running workspaces are forcibly stopped. 

* If your infrastructure does support workspace recovery, any workspaces that are currently starting or stopping is allowed to finish that process. Running workspaces do not stop.

For those that did not stop, automatic fallback to the shutdown with full workspaces stopping will be performed. 

If you want a full shutdown with workspaces stopped, you can request this by using the `shutdown=true` parameter. When preparation process is finished, the `READY_TO_SHUTDOWN` state is set which allows to stop current Che master instance.

[id="che-workspace-termination-grace-period"]
== Che Workspace Termination Grace Period

[IMPORTANT]
====
If the `terminationGracePeriodSeconds` have been explicitly set in the Kubernetes/OpenShift recipe, this environment variable does not override the recipe.
====

The default grace termination period of Kubernetes/OpenShift workspace’s pods is '0'.  This setting terminates pods almost instantly and significantly decreases the time required for stopping a workspace. 

To increase the grace termination period:

* Update `CHE_INFRA_KUBERNETES_POD_TERMINATION__GRACE__PERIOD__SEC`

[id="recreate-update"]
== Updating Che without stopping active workspaces

The differences between a Recreate update and a Rolling update:

Recreate update

* Che downtime

Rolling update

* No Che downtime
* New deployment starts in parallel and traffic is hot-switched

=== Performing a Recreate update

Prerequisites

* Ensure that the new master version is fully API compatible with the old ws agent version.

Procedure

* Set the deployment update strategy to Recreate
* Make POST request to the /api/system/stop api to start WS master suspend (means that all new attempts to start workspaces will be refused, and all current starts/stops will be finished). Note that this method requires system admin credentials.
* Make periodical GET requests to /api/system/state api, until it returns READY_TO_SHUTDOWN state. Also, you can check for "System is ready to shutdown" in the server logs.
* Perform new deploy.

[id="rolling-update"]
=== Performing a Rolling Update

Prerequisites

* Ensure that the new master is fully API compatible with the old ws agent versions, as well as database compatibility (since it is impossible to use DB migrations on this update mode).

Procedure

* Set the deployment update strategy set to Rolling.
* Ensure `terminationGracePeriodSeconds` deployment parameter has enough value (see details below).
* Press *Deploy* button or execute `oc rollout latest che` from cli client.

[id="known-issues"]
==== Known issues

* Workspaces that are started shortly (5-30sec) before the network traffic is switched to the new pod, may fallback to the stopped state. That happens because bootstrappers uses Che server route URL for notifying Che Server when bootstrapping is done. Since traffic is already switched to the new Che server, old one cannot get bootstrapper-s report, and fails the start after waiting timeout reached. If old Che server will be killed before this timeout, workspaces can stuck in the `STARTING` state. So the `terminationGracePeriodSeconds` parameter must define time enough to cover workspace start timeout timeout (which is 8 min by def.) plus some additional timings. Typically, setting `terminationGracePeriodSeconds` to 540 sec is enough to cover all timeouts.
* Some users may experience problems with websocket reconnections or missed events published by WebSocket connection(when a workspace is STARTED but dashboard displays that it is STARTING); Need to reload page to restore connections and actual workspaces states.

[id="update-with-db-migrations-or-api-incompatibility"]
=== Updating with database migrations or API incompatibility

If new version of Che server contains some DB migrations, but there is still API compatibility between old and new version, recreate update type may be used, without stopping running workspaces.

API incompatible versions should be updated with full workspaces stop. It means that `/api/system/stop?shutdown=true` must be called prior to update.

[id="delete-deployments"]
== Deleting deployments

The fastest way to completely delete Che and its infrastructure components is to delete the project and namespace.

To delete Che and components:

`oc delete namespace che`

You can use selectors to delete particular deployments and associated objects.

To remove all Che server related objects:
----
oc delete all -l=app=che
----

To remove all Keycloak related objects
----
oc delete all -l=app=keycloak
----

To remove all PostgreSQL related objects
----
oc delete all -l=app=postgres
----

PVCs, service accounts and role bindings should be deleted separately because `oc delete all` does not delete them.

To delete Che server PVC, ServiceAccount and RoleBinding:
----
oc delete sa -l=app=che
oc delete rolebinding -l=app=che
----

To delete Keycloak and PostgreSQL PVCs
----
oc delete pvc -l=app=keycloak
oc delete pvc -l=app=postgres
----

[id="create-workspace-objects-in-personal-namespaces"]
== Create workspace objects in personal namespaces

You can register the OpenShift server as an identity provider when Che is installed in multi-user mode. This allows you to create workspace objects in the OpenShift namespace of the user that is logged in Che through Keycloak.

To create a workspace object in the namespace of the user that is logged into Che:

* link:#openshift-identity-provider-registration[Register], inside Keycloak, an OpenShift identity provider that points to the OpenShift console of the cluster
* link:#che-configuration[configure] Che to use this Keycloak identity provider in order to retrieve the OpenShift tokens of Che users.

Once this is done, every interactive action done by a Che user on workspaces, such as start or stop, will create OpenShift resources under his personal OpenShift account. And the first time the user will try to do it, he will be asked to link his Keycloak account with his personal OpenShift account: which he can do by simply following the provided link in the notification message.

But for non-interactive workspace actions, such as workspace stop on idling or Che server shutdown, the account used for operations on OpenShift resources will fall back to the dedicated OpenShift account configured for the Kubernetes infrastructure, as described in the link:admin-guide#who-creates-workspace-objects[AdminGuide].

To easily install Che on OpenShift with this feature enabled, see link:openshift-multi-user#creating-workspace-resources-in-personal-openshift-accounts-on-minishift[this section for Minishift] and link:openshift-multi-user#creating-workspace-resources-in-personal-openshift-accounts[this one for OCP]

[id="openshift-identity-provider-registration"]
==== OpenShift identity provider registration

The Keycloak OpenShift identity provider is described in https://www.keycloak.org/docs/3.3/server_admin/topics/identity-broker/social/openshift.html[this documentation].

1.  In the link:user-management#auth-and-user-management[Keycloak administration console], when adding the OpenShift identity provider, you should use the following settings:

image::keycloak/openshift_identity_provider.png[]

`Base URL` is the URL of the OpenShift console

1.  Next thing is to add a default read-token role:

image::git/kc_roles.png[]

1.  Then this identity provider has to be declared as an OAuth client inside OpenShift. This can be done with the corresponding command:

----
oc create -f <(echo '
apiVersion: v1
kind: OAuthClient
metadata:
  name: kc-client
secret: "<value set for the 'Client Secret' field in step 1>"
redirectURIs:
  - "<value provided in the 'Redirect URI' field in step 1>"
grantMethod: prompt
')
----

*Note*: Adding a OAuth client requires cluster-wide admin rights.

[id="che-configuration"]
==== Che configuration

On the Che deployment configuration:

* the `CHE_INFRA_OPENSHIFT_PROJECT` environment variable should be set to `NULL` to ensure a new distinct OpenShift namespace is created for every started workspace.
* the `CHE_INFRA_OPENSHIFT_OAUTH__IDENTITY__PROVIDER` environment variable should be set to the alias of the OpenShift identity provider specified in step 1 of its link:#openshift-identity-provider-registration[registration in Keycloak]. The default value is `openshift-v3`.

[id="providing-the-openshift-certificate-to-keycloak"]
==== Providing the OpenShift certificate to Keycloak

If the certificate used by the OpenShift console is self-signed or is not trusted, then by default the Keycloak will not be able to contact the OpenShift console to retrieve linked tokens.

In this case the OpenShift console certificate should be passed to the Keycloak deployment as an additional environment property. This will enable the Keycloak server to add it to its list of trusted certificates, and will fix the problem.

The environment variable is named `OPENSHIFT_IDENTITY_PROVIDER_CERTIFICATE`.

Since adding a multi-line certificate content in a deployment configuration environment variable is not that easy, the best way is to use a secret that contains the certificate, and refer to it in the environment variable.
